{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624b2398",
   "metadata": {},
   "source": [
    "### Tarefa 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6db874",
   "metadata": {},
   "source": [
    "#### 1. Cite 5 diferenças entre o AdaBoost e o GBM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b244bc0",
   "metadata": {},
   "source": [
    "<span style=\"color:limegreen\">\n",
    "1. Enquanto no AdaBoost, os \"aprendizes fracos\" são representados por árvores de decisão de profundidade 1, no GBM, esses aprendizes podem ser árvores complexas de qualquer profundidade.<br><br>\n",
    "2. O processo inicial no GBM envolve o cálculo da média de Y, enquanto no AdaBoost, é a criação de um stump (árvore rasa).<br><br>\n",
    "3. No AdaBoost, as respostas de cada estimador têm pesos diferentes com base em suas performances, mas no GBM, as estimativas são multiplicadas por uma constante igual para todos.<br><br>\n",
    "4. O AdaBoost treina cada estimador com um conjunto de dados diferente, ao contrário do GBM, que usa todo o conjunto de dados para treinamento de cada árvore.<br><br>\n",
    "5. Enquanto o AdaBoost ensina cada estimador a prever diretamente a variável resposta, o GBM treina cada árvore para estimar o resíduo.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac70a6",
   "metadata": {},
   "source": [
    "#### 2. Acesse o link Scikit-learn – GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63e5638a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo de classificação\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier as gbc\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = gbc(n_estimators=100, learning_rate=1.0, \n",
    "                                 max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ae77ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960319"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbr\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = gbr(n_estimators=100, learning_rate=0.1, \n",
    "          max_depth=1, random_state=0, \n",
    "          loss='squared_error').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251ac4c4",
   "metadata": {},
   "source": [
    "#### 3. Cite 5 Hyperparametros importantes no GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8c440",
   "metadata": {},
   "source": [
    "<span style=\"color:limegreen\">\n",
    "<strong>1. n_estimators:</strong> O número de árvores a serem construídas no modelo. Quanto maior, mais complexo o modelo, porém pode levar a overfitting.<br><br>\n",
    "<strong>2. learning_rate:</strong> Taxa de aprendizado que controla a contribuição de cada árvore ao modelo. Um valor menor requer mais árvores para o mesmo desempenho, mas pode aumentar a robustez.<br><br>\n",
    "<strong>3. max_depth:</strong> A profundidade máxima das árvores de decisão construídas. Controla a complexidade do modelo e pode afetar o overfitting.<br><br>\n",
    "<strong>4. subsample:</strong> A fração dos dados de treinamento usada para construir cada árvore. Pode ajudar a evitar overfitting, mas valores muito baixos podem levar a um viés elevado.<br><br>\n",
    "<strong>5. min_samples_split:</strong> O número mínimo de amostras necessárias para dividir um nó interno da árvore. Controla a quantidade de divisões e pode afetar o overfitting.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26357371",
   "metadata": {},
   "source": [
    "#### 4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75258208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros:\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
      "CPU times: user 4min 15s, sys: 131 ms, total: 4min 15s\n",
      "Wall time: 4min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Carregando os dados\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Definindo os hiperparâmetros para testar\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'min_samples_split': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Criando o modelo\n",
    "model = gbc()\n",
    "\n",
    "# Criando o GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Treinando o modelo usando o GridSearchCV\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Imprimindo os melhores hiperparâmetros encontrados\n",
    "print(\"Melhores hiperparâmetros:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a6d583",
   "metadata": {},
   "source": [
    "#### 5. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79000a8e",
   "metadata": {},
   "source": [
    "<span style=\"color:limegreen\">A maior diferença entre o Gradient Boosting (GBM) e o Stochastic Gradient Boosting (Stochastic GBM) está na forma como eles constroem as árvores. No GBM, todas as observações são usadas para construir cada árvore. Já no Stochastic GBM, a cada iteração, é selecionada uma amostra aleatória das observações para criar a árvore. Isso adiciona um toque de aleatoriedade ao processo, o que ajuda a evitar overfitting e a tornar o modelo mais robusto. Em resumo, o Stochastic GBM introduz um elemento de variação nas árvores, o que pode ser vantajoso para lidar com dados ruidosos ou conjuntos de treinamento grandes.</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
